{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "\n",
    "import datetime\n",
    "import easydict\n",
    "import itertools\n",
    "import json\n",
    "import matplotlib\n",
    "import pathlib\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from operator import itemgetter\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = easydict.EasyDict({\n",
    "    \"raw_train_path\": \"./data/Training\",\n",
    "    \"raw_valid_path\": \"./data/Validation\",\n",
    "    \"clean\": False,\n",
    "    \"data\": \"data\", ## data path\n",
    "    \"seed\": 42,\n",
    "    \"sample_submission_path\": \"sample_submission.tsv\",\n",
    "    \"answer_path\": \"answer.tsv\",\n",
    "    \"prediction_path\": \"prediction.tsv\",\n",
    "    \"pretrained_model_name\": \"gogamza/kobart-base-v1\",\n",
    "    \"train\": \"/workspace/home/uglee/Projects/title_extraction/datasets/integrated_pre_datasets/train_data.tsv\",\n",
    "    \"valid\": \"/workspace/home/uglee/Projects/title_extraction/datasets/integrated_pre_datasets/valid_data.tsv\",\n",
    "    \"test\": \"/workspace/home/uglee/Projects/title_extraction/datasets/integrated_pre_datasets/test_data.tsv\",\n",
    "    ## Training arguments.\n",
    "    \"ckpt\": \"ckpt\", ## path\n",
    "    \"logs\": \"logs\", ## path\n",
    "    \"per_replica_batch_size\": 8,\n",
    "    \"gradient_accumulation_steps\": 16,\n",
    "    \"lr\": 5e-5,\n",
    "    \"weight_decay\": 1e-2,\n",
    "    \"warmup_ratio\": 0.2,\n",
    "    \"n_epochs\": 10,\n",
    "    \"inp_max_len\": 1024,\n",
    "    \"tar_max_len\": 256,\n",
    "    \"model_fpath\": \"/workspace/home/uglee/Projects/title_extraction/src/model_records/kobart-model.pth\",\n",
    "    ## Inference.\n",
    "    \"gpu_id\": 3,\n",
    "    \"beam_size\": 5,\n",
    "    \"length_penalty\": 0.8,\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    \"var_len\": False,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "saved_data = torch.load(\n",
    "    \"/workspace/home/uglee/Projects/title_extraction/src/model_records/kobart-model.pth\",\n",
    "    map_location=\"cpu\" if config.gpu_id < 0 else \"cuda:%d\" % config.gpu_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_best = saved_data[\"model\"]\n",
    "train_config = saved_data[\"config\"]\n",
    "tokenizer = transformers.PreTrainedTokenizerFast.from_pretrained(train_config.pretrained_model_name)\n",
    "\n",
    "## Load weights.\n",
    "model = transformers.BartForConditionalGeneration.from_pretrained(train_config.pretrained_model_name)\n",
    "model.load_state_dict(bart_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing input texts: 100%|██████████| 952/952 [00:00<00:00, 4644.54it/s]\n"
     ]
    }
   ],
   "source": [
    "from dataloaders.dataloader import get_datasets\n",
    "from dataloaders.data_utils import TextAbstractSummarizationCollator\n",
    "\n",
    "\n",
    "ts_ds = get_datasets(tokenizer, fpath=Path(config.test), mode=\"test\")\n",
    "\n",
    "## We will use pytorch dataloader, not huggingface trainer.\n",
    "ts_loader = torch.utils.data.DataLoader(\n",
    "    ts_ds,\n",
    "    batch_size=config.per_replica_batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=TextAbstractSummarizationCollator(\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        inp_max_len=config.inp_max_len,\n",
    "        tar_max_len=config.tar_max_len,\n",
    "        mode=\"test\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/119 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [02:01<00:00,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    if config.gpu_id >= 0:\n",
    "        model.cuda(config.gpu_id)\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    ## Don't forget turn-on evaluation mode.\n",
    "    model.eval()\n",
    "\n",
    "    outputs = []\n",
    "    for mini_batch in tqdm(ts_loader, total=len(ts_loader)):\n",
    "        id = mini_batch[\"id\"]\n",
    "        input_ids = mini_batch[\"input_ids\"]\n",
    "        attention_mask = mini_batch[\"attention_mask\"]\n",
    "\n",
    "        if config.var_len:\n",
    "            ## Variable min, max length of target summaries.\n",
    "            ## We know that summaries ~= text * 0.1.\n",
    "            avg_len = int(input_ids.ne(tokenizer.pad_token_id).view(-1).sum() / input_ids.size(0))\n",
    "            min_length = max(64,  int(avg_len * 0.05))\n",
    "            max_length = min(256, int(avg_len * 0.15))\n",
    "            ## And we don't need to set length penalty anymore.\n",
    "            config.length_penalty = 1.0\n",
    "        else:\n",
    "            min_length = config.tar_max_len // 4 ## maybe, less then 64 (e.g. 48 or 32) can be more score..\n",
    "            max_length = config.tar_max_len\n",
    "\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "        ## Generate ids of summaries.\n",
    "        output = model.generate(\n",
    "            input_ids, \n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length,                  ## maximum summarization size\n",
    "            min_length=min_length,                  ## minimum summarization size\n",
    "            early_stopping=True,                    ## stop the beam search when at least 'num_beams' sentences are finished per batch\n",
    "            num_beams=config.beam_size,             ## beam search size\n",
    "            bos_token_id=tokenizer.bos_token_id,    ## <s> = 0\n",
    "            eos_token_id=tokenizer.eos_token_id,    ## <\\s> = 1\n",
    "            pad_token_id=tokenizer.pad_token_id,    ## 3\n",
    "            length_penalty=config.length_penalty,   ## value > 1.0 in order to encourage the model to produce longer sequences\n",
    "            no_repeat_ngram_size=config.no_repeat_ngram_size,   ## same as 'trigram blocking'\n",
    "        )\n",
    "        ## If you want to decode by each sentence, you may \n",
    "        ## call 'decode' fn, not 'batch_decode'.\n",
    "        output = tokenizer.batch_decode(\n",
    "            output.tolist(), \n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "\n",
    "        ## Get all.\n",
    "        outputs.extend([{\"id\": id_, \"output\": output_} for id_, output_ in zip(id, output)])\n",
    "\n",
    "## Sort and extract.\n",
    "outputs = sorted(\n",
    "    outputs,\n",
    "    key=itemgetter(\"id\"),\n",
    "    reverse=False,\n",
    ")\n",
    "outputs = [i[\"output\"] for i in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "냉이캐기 냉이 냉이이이기캐기?이네 냉이네이이캐김 냉이 두이이 캐기냉이이냉이캐캐기서 냉이김이이 제이이김기이이제 냉이어이이 냉이가  냉이\n"
     ]
    }
   ],
   "source": [
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
