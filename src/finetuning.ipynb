{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "import datetime\n",
    "import easydict\n",
    "import itertools\n",
    "import json\n",
    "import matplotlib\n",
    "import pathlib\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from operator import itemgetter\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = easydict.EasyDict({\n",
    "    \"raw_train_path\": \"./data/Training\",\n",
    "    \"raw_valid_path\": \"./data/Validation\",\n",
    "    \"clean\": False,\n",
    "    \"data\": \"data\", ## data path\n",
    "    \"seed\": 42,\n",
    "    \"sample_submission_path\": \"sample_submission.tsv\",\n",
    "    \"answer_path\": \"answer.tsv\",\n",
    "    \"prediction_path\": \"prediction.tsv\",\n",
    "    \"pretrained_model_name\": \"gogamza/kobart-base-v1\",\n",
    "    \"train\": \"data/train.tsv\",\n",
    "    \"valid\": \"data/valid.tsv\",\n",
    "    \"test\": \"data/test.tsv\",\n",
    "    ## Training arguments.\n",
    "    \"ckpt\": \"ckpt\", ## path\n",
    "    \"logs\": \"logs\", ## path\n",
    "    \"per_replica_batch_size\": 8,\n",
    "    \"gradient_accumulation_steps\": 16,\n",
    "    \"lr\": 5e-5,\n",
    "    \"weight_decay\": 1e-2,\n",
    "    \"warmup_ratio\": 0.2,\n",
    "    \"n_epochs\": 10,\n",
    "    \"inp_max_len\": 1024,\n",
    "    \"tar_max_len\": 256,\n",
    "    \"model_fpath\": \"/content/drive/MyDrive/ColabNotebooks/dacon-gas/kobart-model.pth\",\n",
    "    ## Inference.\n",
    "    \"gpu_id\": 0,\n",
    "    \"beam_size\": 5,\n",
    "    \"length_penalty\": 0.8,\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    \"var_len\": False,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='gogamza/kobart-base-v1', vocab_size=30000, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>'})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = transformers.PreTrainedTokenizerFast.from_pretrained(\n",
    "    'gogamza/kobart-base-v1'\n",
    ")\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   '_additional_special_tokens': [],\n",
      "    '_bos_token': '<s>',\n",
      "    '_cls_token': None,\n",
      "    '_decode_use_source_tokenizer': False,\n",
      "    '_eos_token': '</s>',\n",
      "    '_mask_token': '<mask>',\n",
      "    '_pad_token': '<pad>',\n",
      "    '_pad_token_type_id': 0,\n",
      "    '_sep_token': None,\n",
      "    '_tokenizer': <tokenizers.Tokenizer object at 0xc9d6b50>,\n",
      "    '_unk_token': '<unk>',\n",
      "    'deprecation_warnings': {},\n",
      "    'init_inputs': (),\n",
      "    'init_kwargs': {   'name_or_path': 'gogamza/kobart-base-v1',\n",
      "                       'special_tokens_map_file': '/root/.cache/huggingface/transformers/3e6abf40f4fadbea9e7b539c182868d979838d8f7e6cdcdf2ed52ddcf01420c0.15447ae63ad4a2eba8bc7a5146360711dc32b315b4f1488b4806debf35315e9a'},\n",
      "    'model_input_names': ['input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    'model_max_length': 1000000000000000019884624838656,\n",
      "    'name_or_path': 'gogamza/kobart-base-v1',\n",
      "    'padding_side': 'right',\n",
      "    'verbose': True}\n"
     ]
    }
   ],
   "source": [
    "def print_elements(a: dict) -> None:\n",
    "    pprint.PrettyPrinter(indent=4).pprint(a)\n",
    "\n",
    "print_elements(vars(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_list = os.listdir(folder_path)\n",
    "\n",
    "def read_tsv(fpath: pathlib.PosixPath) -> pd.DataFrame:\n",
    "    return pd.read_csv(\n",
    "        fpath, \n",
    "        index_col=False,\n",
    "        names=['summary', 'text'],\n",
    "        sep=\"\\t\",\n",
    "        encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class TextAbstractSummarizationDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        fpath: pathlib.PosixPath,\n",
    "        mode: str = \"train\",\n",
    "    ):\n",
    "        super(TextAbstractSummarizationDataset, self).__init__()\n",
    "\n",
    "        self.df = read_tsv(fpath)\n",
    "        # self.tok = tokenizer -> don't keep\n",
    "        self.df.loc[:, \"id\"] = [i for i in range(self.df.shape[0])]\n",
    "        \n",
    "        ## Mode.\n",
    "        assert mode in [\"train\", \"test\"]\n",
    "        self.mode = mode\n",
    "\n",
    "        ## Apply tokenize first to speed up in training phase and make code more simply.\n",
    "        tqdm.pandas(desc=\"Tokenizing input texts\")\n",
    "        self.df.loc[:, \"text_tok\"] = self.df.loc[:, \"text\"].progress_apply(lambda x: tokenizer.encode(x))\n",
    "        self.df.loc[:, \"text_tok_len\"] = self.df.loc[:, \"text_tok\"].apply(lambda x: len(x))\n",
    "\n",
    "        if self.mode == \"train\":\n",
    "            tqdm.pandas(desc=\"Tokenizing target summaries\")\n",
    "            self.df.loc[:, \"summary_tok\"] = self.df.loc[:, \"summary\"].progress_apply(lambda x: tokenizer.encode(x))\n",
    "            self.df.loc[:, \"summary_tok_len\"] = self.df.loc[:, \"summary_tok\"].apply(lambda x: len(x))\n",
    "\n",
    "        ## Sort by tokenized length with tqdm progress bar.\n",
    "        ## \n",
    "        ## By sorting sequentially, starting with the longest sentence, \n",
    "        ## we can determine the maximum VRAM size the model is using for\n",
    "        ## training. That is, if OOM does not occur for the maximum VRAM\n",
    "        ## size at the beginning of training, it is guaranteed that OOM\n",
    "        ## does not occur during training.\n",
    "        self.df.sort_values(by=[\"text_tok_len\"], axis=0, ascending=False, inplace=True)\n",
    "\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.df.shape[0]\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, List[int]]:\n",
    "        instance = self.df.iloc[idx]\n",
    "\n",
    "        return_value = {\n",
    "            \"id\": instance[\"id\"], ## for sorting in inference mode\n",
    "            \"text\": instance[\"text_tok\"],\n",
    "            \"length\": len(instance[\"text_tok\"]),\n",
    "        }\n",
    "        if self.mode == \"train\":\n",
    "            return_value[\"summary\"] = instance[\"summary_tok\"]\n",
    "        \n",
    "        return return_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextAbstractSummarizationCollator():\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        bos_token_id: int,\n",
    "        eos_token_id: int,\n",
    "        pad_token_id: int,\n",
    "        inp_max_len: int = 1024,\n",
    "        tar_max_len: int = 256,\n",
    "        ignore_index: int = -100,\n",
    "        mode: str = \"train\",\n",
    "    ):\n",
    "        super(TextAbstractSummarizationCollator, self).__init__()\n",
    "\n",
    "        self.bos_token_id = bos_token_id\n",
    "        self.eos_token_id = eos_token_id\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.inp_max_len = inp_max_len\n",
    "        self.tar_max_len = tar_max_len\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "        ## Mode.\n",
    "        assert mode in [\"train\", \"test\"]\n",
    "        self.mode = mode\n",
    "\n",
    "\n",
    "    def _pad(self, sentences: List[List[int]], token_id: int) -> np.ndarray:\n",
    "        ## We will pad as max length per batch, not \"inp_max_len(=1024, etc)\".\n",
    "        max_length_per_batch = max([len(i) for i in sentences])\n",
    "\n",
    "        ## Stack as dimension 0 (batch dimension).\n",
    "        ## \"token_id\" can be \"tokenizer.pad_token_id(=3)\" or \"ignore_index(=-100)\"\n",
    "        return np.stack([i + [token_id] * (max_length_per_batch - len(i)) for i in sentences], axis=0)\n",
    "\n",
    "\n",
    "    def _train_collator(self, samples: List[Dict[str, List[int]]]) -> Dict[str, List[int]]:\n",
    "        ## Unpack.\n",
    "\n",
    "        ## If input max length > 1024, you can see below error:\n",
    "        ##   1) Assertion `srcIndex < srcSelectDimSize` failed\n",
    "        ##   2) Device-side assert triggered\n",
    "        tokenized_texts     = [s[\"text\"][:self.inp_max_len]        for s in samples]\n",
    "        tokenized_summaries = [s[\"summary\"][:self.tar_max_len - 1] for s in samples] ## <bos> or <eos> token index\n",
    "\n",
    "        ## Inputs for encoder.\n",
    "        input_ids = self._pad(tokenized_texts, token_id=self.pad_token_id)  ## numpy format\n",
    "        attention_mask = (input_ids != self.pad_token_id).astype(float)     ## numpy format\n",
    "\n",
    "        ## Inputs for decoder (generator).\n",
    "        decoder_input_ids = [[self.bos_token_id] + i for i in tokenized_summaries]      ## bos\n",
    "        decoder_input_ids = self._pad(decoder_input_ids, token_id=self.pad_token_id)    ## eos\n",
    "        decoder_attention_mask = (decoder_input_ids != self.pad_token_id).astype(float)\n",
    "\n",
    "        ## Answer.\n",
    "        labels = [i + [self.eos_token_id] for i in tokenized_summaries]\n",
    "        labels = self._pad(labels, token_id=self.ignore_index) ## why != \"padding_id\" ???\n",
    "\n",
    "        ## We ensure that generator's inputs' and outputs' shapes are equal.\n",
    "        assert decoder_input_ids.shape == labels.shape\n",
    "        \n",
    "        ## Pack as pre-defined arguments. See:\n",
    "        ##   https://huggingface.co/docs/transformers/model_doc/bart#transformers.BartForConditionalGeneration\n",
    "        return {\n",
    "            \"input_ids\":                torch.from_numpy(input_ids),\n",
    "            \"attention_mask\":           torch.from_numpy(attention_mask),\n",
    "            \"decoder_input_ids\":        torch.from_numpy(decoder_input_ids),\n",
    "            \"decoder_attention_mask\":   torch.from_numpy(decoder_attention_mask),\n",
    "            \"labels\":                   torch.from_numpy(labels),\n",
    "        }\n",
    "\n",
    "\n",
    "    def _test_collator(self, samples: List[Dict[str, List[int]]]) -> Dict[str, List[int]]:\n",
    "        ## Unpack.\n",
    "        ids              = [s[\"id\"]                      for s in samples]\n",
    "        tokenized_texts  = [s[\"text\"][:self.inp_max_len] for s in samples]   ## no <bos> token included\n",
    "\n",
    "        ## Inputs for encoder.\n",
    "        input_ids = self._pad(tokenized_texts, token_id=self.pad_token_id)  ## numpy format\n",
    "        attention_mask = (input_ids != self.pad_token_id).astype(float)     ## numpy format\n",
    "\n",
    "        ## Pack as pre-defined arguments:\n",
    "        ## See: https://huggingface.co/docs/transformers/model_doc/bart#transformers.BartForConditionalGeneration\n",
    "        return {\n",
    "            \"input_ids\":        torch.from_numpy(input_ids),\n",
    "            \"attention_mask\":   torch.from_numpy(attention_mask),\n",
    "            ## Additional information to make answer.\n",
    "            \"id\":               ids,\n",
    "        }\n",
    "\n",
    "\n",
    "    def __call__(self, samples: List[Dict[str, List[int]]]) -> Dict[str, List[int]]:\n",
    "        return self._train_collator(samples) if self.mode == \"train\" else self._test_collator(samples)\n",
    "\n",
    "\n",
    "def get_datasets(tokenizer, fpath: pathlib.PosixPath, mode: str = \"train\"):\n",
    "    return TextAbstractSummarizationDataset(tokenizer, fpath, mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing input texts: 100%|██████████| 952/952 [00:00<00:00, 4228.63it/s]\n",
      "Tokenizing target summaries: 100%|██████████| 952/952 [00:00<00:00, 18920.30it/s]\n",
      "Tokenizing input texts: 100%|██████████| 952/952 [00:00<00:00, 4994.40it/s]\n",
      "Tokenizing target summaries: 100%|██████████| 952/952 [00:00<00:00, 18471.04it/s]\n"
     ]
    }
   ],
   "source": [
    "tr_ds = get_datasets(\n",
    "    tokenizer=tokenizer,\n",
    "    fpath=Path(\"/workspace/home/uglee/Projects/title_extraction/datasets/integrated_pre_datasets/train_data.tsv\")\n",
    ")\n",
    "\n",
    "vl_ds = get_datasets(\n",
    "    tokenizer=tokenizer,\n",
    "    fpath=Path(\"/workspace/home/uglee/Projects/title_extraction/datasets/integrated_pre_datasets/valid_data.tsv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(952, 952)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr_ds), len(vl_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/gogamza/kobart-base-v1/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/000a9f24bd41cd04e1263037d6b84bbc25b57d0f00a19f7540099c8a448cfeec.06f809b56ae6e8b04642a7106106b25f3e00faae652a6ee84df5fd19c5804ee2\n",
      "Model config BartConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 1.0,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/gogamza/kobart-base-v1/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/247767aee78d1fc438bbb3a5dcaf03e4687d4b5129a0bf0e516d1a7409091074.779a34a2d0bb40d966554988434f63315eaf2a8cf92c643132e3a2b48ddc5e55\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at gogamza/kobart-base-v1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BartForConditionalGeneration.from_pretrained(\n",
    "    'gogamza/kobart-base-v1'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   '__cached__setup_devices': device(type='cuda', index=0),\n",
      "    '_n_gpu': 4,\n",
      "    'adafactor': False,\n",
      "    'adam_beta1': 0.9,\n",
      "    'adam_beta2': 0.999,\n",
      "    'adam_epsilon': 1e-08,\n",
      "    'dataloader_drop_last': False,\n",
      "    'dataloader_num_workers': 4,\n",
      "    'dataloader_pin_memory': True,\n",
      "    'ddp_find_unused_parameters': None,\n",
      "    'debug': [],\n",
      "    'deepspeed': None,\n",
      "    'disable_tqdm': False,\n",
      "    'do_eval': True,\n",
      "    'do_predict': False,\n",
      "    'do_train': False,\n",
      "    'eval_accumulation_steps': None,\n",
      "    'eval_steps': None,\n",
      "    'evaluation_strategy': <IntervalStrategy.EPOCH: 'epoch'>,\n",
      "    'fp16': True,\n",
      "    'fp16_backend': 'auto',\n",
      "    'fp16_full_eval': False,\n",
      "    'fp16_opt_level': 'O1',\n",
      "    'generation_max_length': None,\n",
      "    'generation_num_beams': None,\n",
      "    'gradient_accumulation_steps': 16,\n",
      "    'gradient_checkpointing': False,\n",
      "    'greater_is_better': False,\n",
      "    'group_by_length': False,\n",
      "    'hub_model_id': None,\n",
      "    'hub_strategy': <HubStrategy.EVERY_SAVE: 'every_save'>,\n",
      "    'hub_token': None,\n",
      "    'ignore_data_skip': False,\n",
      "    'label_names': None,\n",
      "    'label_smoothing_factor': 0.0,\n",
      "    'learning_rate': 5e-05,\n",
      "    'length_column_name': 'length',\n",
      "    'load_best_model_at_end': True,\n",
      "    'local_rank': -1,\n",
      "    'log_level': -1,\n",
      "    'log_level_replica': -1,\n",
      "    'log_on_each_node': True,\n",
      "    'logging_dir': 'logs/20230116-013603/run',\n",
      "    'logging_first_step': False,\n",
      "    'logging_nan_inf_filter': True,\n",
      "    'logging_steps': 10,\n",
      "    'logging_strategy': <IntervalStrategy.STEPS: 'steps'>,\n",
      "    'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>,\n",
      "    'max_grad_norm': 1.0,\n",
      "    'max_steps': -1,\n",
      "    'metric_for_best_model': 'loss',\n",
      "    'mp_parameters': '',\n",
      "    'no_cuda': False,\n",
      "    'num_train_epochs': 10,\n",
      "    'output_dir': 'ckpt/20230116-013603',\n",
      "    'overwrite_output_dir': False,\n",
      "    'past_index': -1,\n",
      "    'per_device_eval_batch_size': 8,\n",
      "    'per_device_train_batch_size': 8,\n",
      "    'per_gpu_eval_batch_size': None,\n",
      "    'per_gpu_train_batch_size': None,\n",
      "    'predict_with_generate': False,\n",
      "    'prediction_loss_only': False,\n",
      "    'push_to_hub': False,\n",
      "    'push_to_hub_model_id': None,\n",
      "    'push_to_hub_organization': None,\n",
      "    'push_to_hub_token': None,\n",
      "    'remove_unused_columns': True,\n",
      "    'report_to': ['tensorboard'],\n",
      "    'resume_from_checkpoint': None,\n",
      "    'run_name': 'ckpt/20230116-013603',\n",
      "    'save_on_each_node': False,\n",
      "    'save_steps': 500,\n",
      "    'save_strategy': <IntervalStrategy.EPOCH: 'epoch'>,\n",
      "    'save_total_limit': None,\n",
      "    'seed': 42,\n",
      "    'sharded_ddp': [],\n",
      "    'skip_memory_metrics': True,\n",
      "    'sortish_sampler': False,\n",
      "    'tpu_metrics_debug': False,\n",
      "    'tpu_num_cores': None,\n",
      "    'use_legacy_prediction_loop': False,\n",
      "    'warmup_ratio': 0.2,\n",
      "    'warmup_steps': 0,\n",
      "    'weight_decay': 0.01,\n",
      "    'xpu_backend': None}\n"
     ]
    }
   ],
   "source": [
    "## Path arguments.\n",
    "nowtime = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "output_dir = Path(config.ckpt, nowtime)\n",
    "logging_dir = Path(config.logs, nowtime, \"run\")\n",
    "\n",
    "training_args = transformers.Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=config.per_replica_batch_size,\n",
    "    per_device_eval_batch_size=config.per_replica_batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    learning_rate=config.lr,\n",
    "    weight_decay=config.weight_decay,\n",
    "    warmup_ratio=config.warmup_ratio,\n",
    "    num_train_epochs=config.n_epochs,\n",
    "    logging_dir=logging_dir,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    # save_steps=1000,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=4,\n",
    "    disable_tqdm=False,\n",
    "    load_best_model_at_end=True,\n",
    "    ## As below, only Seq2SeqTrainingArguments' arguments.\n",
    "    # sortish_sampler=True,\n",
    "    # predict_with_generate=True,\n",
    "    # generation_max_length=config.tar_max_len,   ## 256\n",
    "    # generation_num_beams=config.beam_size,      ## 5\n",
    ")\n",
    "\n",
    "## Print elements.\n",
    "print_elements(vars(training_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n"
     ]
    }
   ],
   "source": [
    "## Define trainer.\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=TextAbstractSummarizationCollator(\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        inp_max_len=config.inp_max_len,\n",
    "        tar_max_len=config.tar_max_len,\n",
    "    ),\n",
    "    train_dataset=tr_ds,\n",
    "    eval_dataset=vl_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 952\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 04:08, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>12.833954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.016085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.275976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.131680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.002076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.753600</td>\n",
       "      <td>0.000034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 952\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ckpt/20230116-013603/checkpoint-1\n",
      "Configuration saved in ckpt/20230116-013603/checkpoint-1/config.json\n",
      "Model weights saved in ckpt/20230116-013603/checkpoint-1/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 952\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ckpt/20230116-013603/checkpoint-2\n",
      "Configuration saved in ckpt/20230116-013603/checkpoint-2/config.json\n",
      "Model weights saved in ckpt/20230116-013603/checkpoint-2/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 952\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ckpt/20230116-013603/checkpoint-3\n",
      "Configuration saved in ckpt/20230116-013603/checkpoint-3/config.json\n",
      "Model weights saved in ckpt/20230116-013603/checkpoint-3/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 952\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ckpt/20230116-013603/checkpoint-4\n",
      "Configuration saved in ckpt/20230116-013603/checkpoint-4/config.json\n",
      "Model weights saved in ckpt/20230116-013603/checkpoint-4/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 952\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ckpt/20230116-013603/checkpoint-5\n",
      "Configuration saved in ckpt/20230116-013603/checkpoint-5/config.json\n",
      "Model weights saved in ckpt/20230116-013603/checkpoint-5/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 952\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ckpt/20230116-013603/checkpoint-6\n",
      "Configuration saved in ckpt/20230116-013603/checkpoint-6/config.json\n",
      "Model weights saved in ckpt/20230116-013603/checkpoint-6/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 952\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ckpt/20230116-013603/checkpoint-7\n",
      "Configuration saved in ckpt/20230116-013603/checkpoint-7/config.json\n",
      "Model weights saved in ckpt/20230116-013603/checkpoint-7/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 952\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ckpt/20230116-013603/checkpoint-8\n",
      "Configuration saved in ckpt/20230116-013603/checkpoint-8/config.json\n",
      "Model weights saved in ckpt/20230116-013603/checkpoint-8/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 952\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ckpt/20230116-013603/checkpoint-9\n",
      "Configuration saved in ckpt/20230116-013603/checkpoint-9/config.json\n",
      "Model weights saved in ckpt/20230116-013603/checkpoint-9/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 952\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ckpt/20230116-013603/checkpoint-10\n",
      "Configuration saved in ckpt/20230116-013603/checkpoint-10/config.json\n",
      "Model weights saved in ckpt/20230116-013603/checkpoint-10/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ckpt/20230116-013603/checkpoint-10 (score: 3.35580189130269e-05).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=4.753628540039062, metrics={'train_runtime': 270.6545, 'train_samples_per_second': 35.174, 'train_steps_per_second': 0.037, 'total_flos': 524444807577600.0, 'train_loss': 4.753628540039062, 'epoch': 9.53})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Just train.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
