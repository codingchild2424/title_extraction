{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "\n",
    "import datetime\n",
    "import easydict\n",
    "import itertools\n",
    "import json\n",
    "import matplotlib\n",
    "import pathlib\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from operator import itemgetter\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = easydict.EasyDict({\n",
    "\n",
    "    'gpu_id': 3,\n",
    "    'train_data_path': '../datasets/integrated_pre_datasets/train_data.tsv',\n",
    "    'valid_data_path': '../datasets/integrated_pre_datasets/valid_data.tsv',\n",
    "    'test_data_path': '../datasets/integrated_pre_datasets/test_data.tsv',\n",
    "    'pretrained_model_name': 'gogamza/kobart-base-v1',\n",
    "    'ckpt': \"ckpt\",\n",
    "    'logs': \"logs\",\n",
    "    'batch_size_per_device': 8, # 8이 최선\n",
    "    'gradient_accumulation_steps': 16,\n",
    "    'lr': 5e-5,\n",
    "    'weight_decay': 1e-2,\n",
    "    'warmup_ratio': .2,\n",
    "    'n_epochs': 10,\n",
    "    'inp_max_len': 1024,\n",
    "    'tar_max_len': 256,\n",
    "    'model_fpath': \"model_records/kobart-model.pth\",\n",
    "    'beam_size': 5,\n",
    "    'var_len': False,\n",
    "    \"length_penalty\": 0.8,\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "saved_data = torch.load(\n",
    "    \"/workspace/home/uglee/Projects/title_extraction/src/model_records/kobart-model.pth\",\n",
    "    map_location=\"cpu\" if config.gpu_id < 0 else \"cuda:%d\" % config.gpu_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_best = saved_data[\"model\"]\n",
    "train_config = saved_data[\"config\"]\n",
    "tokenizer = transformers.PreTrainedTokenizerFast.from_pretrained(train_config.pretrained_model_name)\n",
    "\n",
    "## Load weights.\n",
    "model = transformers.BartForConditionalGeneration.from_pretrained(train_config.pretrained_model_name)\n",
    "model.load_state_dict(bart_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing input texts: 100%|██████████| 3052/3052 [00:01<00:00, 1938.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from dataloaders.bart_dataloader import get_datasets\n",
    "from dataloaders.bart_dataloader import TextAbstractSummarizationCollator\n",
    "\n",
    "ts_ds = get_datasets(tokenizer, fpath=Path(config.test_data_path), mode=\"test\")\n",
    "\n",
    "ts_loader = torch.utils.data.DataLoader(\n",
    "    ts_ds,\n",
    "    batch_size=config.batch_size_per_device,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=TextAbstractSummarizationCollator(\n",
    "        tokenizer=tokenizer,\n",
    "        config=config,\n",
    "        mode=\"test\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/382 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'EasyDict' object has no attribute 'no_repeat_ngram_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/workspace/home/uglee/Projects/title_extraction/src/bart_inference.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7079746f7263685f646576222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7562756e7475227d7d/workspace/home/uglee/Projects/title_extraction/src/bart_inference.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m attention_mask \u001b[39m=\u001b[39m attention_mask\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7079746f7263685f646576222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7562756e7475227d7d/workspace/home/uglee/Projects/title_extraction/src/bart_inference.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m## Generate ids of summaries.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7079746f7263685f646576222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7562756e7475227d7d/workspace/home/uglee/Projects/title_extraction/src/bart_inference.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7079746f7263685f646576222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7562756e7475227d7d/workspace/home/uglee/Projects/title_extraction/src/bart_inference.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     input_ids, \n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7079746f7263685f646576222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7562756e7475227d7d/workspace/home/uglee/Projects/title_extraction/src/bart_inference.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7079746f7263685f646576222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7562756e7475227d7d/workspace/home/uglee/Projects/title_extraction/src/bart_inference.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     max_length\u001b[39m=\u001b[39mmax_length,                  \u001b[39m## maximum summarization size\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7079746f7263685f646576222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7562756e7475227d7d/workspace/home/uglee/Projects/title_extraction/src/bart_inference.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     min_length\u001b[39m=\u001b[39mmin_length,                  \u001b[39m## minimum summarization size\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7079746f7263685f646576222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7562756e7475227d7d/workspace/home/uglee/Projects/title_extraction/src/bart_inference.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m     early_stopping\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,                    \u001b[39m## stop the beam search when at least 'num_beams' sentences are finished per batch\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7079746f7263685f646576222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7562756e7475227d7d/workspace/home/uglee/Projects/title_extraction/src/bart_inference.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m     num_beams\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mbeam_size,             \u001b[39m## beam search size\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7079746f7263685f646576222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7562756e7475227d7d/workspace/home/uglee/Projects/title_extraction/src/bart_inference.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m     bos_token_id\u001b[39m=\u001b[39mtokenizer\u001b[39m.\u001b[39mbos_token_id,    \u001b[39m## <s> = 0\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7079746f7263685f646576222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7562756e7475227d7d/workspace/home/uglee/Projects/title_extraction/src/bart_inference.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     eos_token_id\u001b[39m=\u001b[39mtokenizer\u001b[39m.\u001b[39meos_token_id,    \u001b[39m## <\\s> = 1\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7079746f7263685f646576222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7562756e7475227d7d/workspace/home/uglee/Projects/title_extraction/src/bart_inference.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m     pad_token_id\u001b[39m=\u001b[39mtokenizer\u001b[39m.\u001b[39mpad_token_id,    \u001b[39m## 3\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7079746f7263685f646576222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7562756e7475227d7d/workspace/home/uglee/Projects/title_extraction/src/bart_inference.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m     length_penalty\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mlength_penalty,   \u001b[39m## value > 1.0 in order to encourage the model to produce longer sequences\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7079746f7263685f646576222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7562756e7475227d7d/workspace/home/uglee/Projects/title_extraction/src/bart_inference.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m     no_repeat_ngram_size\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39;49mno_repeat_ngram_size,   \u001b[39m## same as 'trigram blocking'\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7079746f7263685f646576222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7562756e7475227d7d/workspace/home/uglee/Projects/title_extraction/src/bart_inference.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7079746f7263685f646576222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7562756e7475227d7d/workspace/home/uglee/Projects/title_extraction/src/bart_inference.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m## If you want to decode by each sentence, you may \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7079746f7263685f646576222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7562756e7475227d7d/workspace/home/uglee/Projects/title_extraction/src/bart_inference.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m## call 'decode' fn, not 'batch_decode'.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7079746f7263685f646576222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7562756e7475227d7d/workspace/home/uglee/Projects/title_extraction/src/bart_inference.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m output \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7079746f7263685f646576222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7562756e7475227d7d/workspace/home/uglee/Projects/title_extraction/src/bart_inference.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m     output\u001b[39m.\u001b[39mtolist(), \n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7079746f7263685f646576222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7562756e7475227d7d/workspace/home/uglee/Projects/title_extraction/src/bart_inference.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m     skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7079746f7263685f646576222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7562756e7475227d7d/workspace/home/uglee/Projects/title_extraction/src/bart_inference.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EasyDict' object has no attribute 'no_repeat_ngram_size'"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    if config.gpu_id >= 0:\n",
    "        model.cuda(config.gpu_id)\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    ## Don't forget turn-on evaluation mode.\n",
    "    model.eval()\n",
    "\n",
    "    outputs = []\n",
    "    for mini_batch in tqdm(ts_loader, total=len(ts_loader)):\n",
    "        id = mini_batch[\"id\"]\n",
    "        input_ids = mini_batch[\"input_ids\"]\n",
    "        attention_mask = mini_batch[\"attention_mask\"]\n",
    "\n",
    "        if config.var_len:\n",
    "            ## Variable min, max length of target summaries.\n",
    "            ## We know that summaries ~= text * 0.1.\n",
    "            avg_len = int(input_ids.ne(tokenizer.pad_token_id).view(-1).sum() / input_ids.size(0))\n",
    "            min_length = max(64,  int(avg_len * 0.05))\n",
    "            max_length = min(256, int(avg_len * 0.15))\n",
    "            ## And we don't need to set length penalty anymore.\n",
    "            config.length_penalty = 1.0\n",
    "        else:\n",
    "            min_length = config.tar_max_len // 4 ## maybe, less then 64 (e.g. 48 or 32) can be more score..\n",
    "            max_length = config.tar_max_len\n",
    "\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "        ## Generate ids of summaries.\n",
    "        output = model.generate(\n",
    "            input_ids, \n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length,                  ## maximum summarization size\n",
    "            min_length=min_length,                  ## minimum summarization size\n",
    "            early_stopping=True,                    ## stop the beam search when at least 'num_beams' sentences are finished per batch\n",
    "            num_beams=config.beam_size,             ## beam search size\n",
    "            bos_token_id=tokenizer.bos_token_id,    ## <s> = 0\n",
    "            eos_token_id=tokenizer.eos_token_id,    ## <\\s> = 1\n",
    "            pad_token_id=tokenizer.pad_token_id,    ## 3\n",
    "            length_penalty=config.length_penalty,   ## value > 1.0 in order to encourage the model to produce longer sequences\n",
    "            no_repeat_ngram_size=config.no_repeat_ngram_size,   ## same as 'trigram blocking'\n",
    "        )\n",
    "        ## If you want to decode by each sentence, you may \n",
    "        ## call 'decode' fn, not 'batch_decode'.\n",
    "        output = tokenizer.batch_decode(\n",
    "            output.tolist(), \n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "\n",
    "        ## Get all.\n",
    "        outputs.extend([{\"id\": id_, \"output\": output_} for id_, output_ in zip(id, output)])\n",
    "\n",
    "## Sort and extract.\n",
    "outputs = sorted(\n",
    "    outputs,\n",
    "    key=itemgetter(\"id\"),\n",
    "    reverse=False,\n",
    ")\n",
    "outputs = [i[\"output\"] for i in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "냉이캐기 냉이 냉이이이기캐기?이네 냉이네이이캐김 냉이 두이이 캐기냉이이냉이캐캐기서 냉이김이이 제이이김기이이제 냉이어이이 냉이가  냉이\n"
     ]
    }
   ],
   "source": [
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
