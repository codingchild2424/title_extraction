{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "\n",
    "import datetime\n",
    "import easydict\n",
    "import itertools\n",
    "import json\n",
    "import matplotlib\n",
    "import pathlib\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from operator import itemgetter\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = easydict.EasyDict({\n",
    "\n",
    "    'gpu_id': 3,\n",
    "    'train_data_path': '../datasets/integrated_pre_datasets/train_data.tsv',\n",
    "    'valid_data_path': '../datasets/integrated_pre_datasets/valid_data.tsv',\n",
    "    'test_data_path': '../datasets/integrated_pre_datasets/test_data.tsv',\n",
    "    'pretrained_model_name': 'gogamza/kobart-base-v1',\n",
    "    'ckpt': \"ckpt\",\n",
    "    'logs': \"logs\",\n",
    "    'batch_size_per_device': 8, # 8이 최선\n",
    "    'gradient_accumulation_steps': 16,\n",
    "    'lr': 5e-5,\n",
    "    'weight_decay': 1e-2,\n",
    "    'warmup_ratio': .2,\n",
    "    'n_epochs': 10,\n",
    "    'inp_max_len': 1024,\n",
    "    'tar_max_len': 256,\n",
    "    'model_fpath': \"model_records/kobart-model2.pth\",\n",
    "    'beam_size': 5,\n",
    "    'var_len': False,\n",
    "    \"length_penalty\": 0.8,\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "saved_data = torch.load(\n",
    "    \"/workspace/home/uglee/Projects/title_extraction/src/model_records/kobart-model.pth\",\n",
    "    map_location=\"cpu\" if config.gpu_id < 0 else \"cuda:%d\" % config.gpu_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_best = saved_data[\"model\"]\n",
    "train_config = saved_data[\"config\"]\n",
    "tokenizer = transformers.PreTrainedTokenizerFast.from_pretrained(train_config.pretrained_model_name)\n",
    "\n",
    "## Load weights.\n",
    "model = transformers.BartForConditionalGeneration.from_pretrained(train_config.pretrained_model_name)\n",
    "model.load_state_dict(bart_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing input texts: 100%|██████████| 3791/3791 [00:03<00:00, 1082.17it/s]\n"
     ]
    }
   ],
   "source": [
    "from dataloaders.bart_dataloader import get_datasets\n",
    "from dataloaders.bart_dataloader import TextAbstractSummarizationCollator\n",
    "\n",
    "ts_ds = get_datasets(tokenizer, fpath=Path(config.test_data_path), mode=\"test\")\n",
    "\n",
    "ts_loader = torch.utils.data.DataLoader(\n",
    "    ts_ds,\n",
    "    batch_size=config.batch_size_per_device,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=TextAbstractSummarizationCollator(\n",
    "        tokenizer=tokenizer,\n",
    "        config=config,\n",
    "        mode=\"test\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 474/474 [08:52<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    if config.gpu_id >= 0:\n",
    "        model.cuda(config.gpu_id)\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    ## Don't forget turn-on evaluation mode.\n",
    "    model.eval()\n",
    "\n",
    "    outputs = []\n",
    "    for mini_batch in tqdm(ts_loader, total=len(ts_loader)):\n",
    "        id = mini_batch[\"id\"]\n",
    "        input_ids = mini_batch[\"input_ids\"]\n",
    "        attention_mask = mini_batch[\"attention_mask\"]\n",
    "\n",
    "        if config.var_len:\n",
    "            ## Variable min, max length of target summaries.\n",
    "            ## We know that summaries ~= text * 0.1.\n",
    "            avg_len = int(input_ids.ne(tokenizer.pad_token_id).view(-1).sum() / input_ids.size(0))\n",
    "            min_length = max(64,  int(avg_len * 0.05))\n",
    "            max_length = min(256, int(avg_len * 0.15))\n",
    "            ## And we don't need to set length penalty anymore.\n",
    "            config.length_penalty = 1.0\n",
    "        else:\n",
    "            min_length = config.tar_max_len // 4 ## maybe, less then 64 (e.g. 48 or 32) can be more score..\n",
    "            max_length = config.tar_max_len\n",
    "\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "        ## Generate ids of summaries.\n",
    "        output = model.generate(\n",
    "            input_ids, \n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length,                  ## maximum summarization size\n",
    "            min_length=min_length,                  ## minimum summarization size\n",
    "            early_stopping=True,                    ## stop the beam search when at least 'num_beams' sentences are finished per batch\n",
    "            num_beams=config.beam_size,             ## beam search size\n",
    "            bos_token_id=tokenizer.bos_token_id,    ## <s> = 0\n",
    "            eos_token_id=tokenizer.eos_token_id,    ## <\\s> = 1\n",
    "            pad_token_id=tokenizer.pad_token_id,    ## 3\n",
    "            length_penalty=config.length_penalty,   ## value > 1.0 in order to encourage the model to produce longer sequences\n",
    "            no_repeat_ngram_size=config.no_repeat_ngram_size,   ## same as 'trigram blocking'\n",
    "        )\n",
    "        ## If you want to decode by each sentence, you may \n",
    "        ## call 'decode' fn, not 'batch_decode'.\n",
    "        output = tokenizer.batch_decode(\n",
    "            output.tolist(), \n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "\n",
    "        ## Get all.\n",
    "        outputs.extend([{\"id\": id_, \"output\": output_} for id_, output_ in zip(id, output)])\n",
    "\n",
    "## Sort and extract.\n",
    "outputs = sorted(\n",
    "    outputs,\n",
    "    key=itemgetter(\"id\"),\n",
    "    reverse=False,\n",
    ")\n",
    "outputs = [i[\"output\"] for i in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "온 가족이 온다  온 온 온다는 것은 온 온도 온도가 온다는 것을 의미한다 온 온도는 온 온도를 온 온도와 온도로 온도온도로 온다는 사실을 의미한다온 온 온전한 온 온온 온온온 온오프 온오프는 온 온오프 모두 온오프를 온오프로 온오프\n"
     ]
    }
   ],
   "source": [
    "print(output[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Mar 15 2022, 12:22:08) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
