{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "\n",
    "import datetime\n",
    "import easydict\n",
    "import itertools\n",
    "import json\n",
    "import matplotlib\n",
    "import pathlib\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from operator import itemgetter\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = easydict.EasyDict({\n",
    "\n",
    "    'gpu_id': 3,\n",
    "    'train_data_path': '../datasets/integrated_pre_datasets/train_data.tsv',\n",
    "    'valid_data_path': '../datasets/integrated_pre_datasets/valid_data.tsv',\n",
    "    'test_data_path': '../datasets/integrated_pre_datasets/test_data.tsv',\n",
    "    'pretrained_model_name': 'gogamza/kobart-base-v1',\n",
    "    'ckpt': \"ckpt\",\n",
    "    'logs': \"logs\",\n",
    "    'batch_size_per_device': 8, # 8이 최선\n",
    "    'gradient_accumulation_steps': 16,\n",
    "    'lr': 5e-5,\n",
    "    'weight_decay': 1e-2,\n",
    "    'warmup_ratio': .2,\n",
    "    'n_epochs': 10,\n",
    "    'inp_max_len': 1024,\n",
    "    'tar_max_len': 256,\n",
    "    'model_fpath': \"model_records/kobart-model.pth\",\n",
    "    'beam_size': 5,\n",
    "    'var_len': False,\n",
    "    \"length_penalty\": 0.8,\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "saved_data = torch.load(\n",
    "    \"/workspace/home/uglee/Projects/title_extraction/src/model_records/kobart-model.pth\",\n",
    "    map_location=\"cpu\" if config.gpu_id < 0 else \"cuda:%d\" % config.gpu_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_best = saved_data[\"model\"]\n",
    "train_config = saved_data[\"config\"]\n",
    "tokenizer = transformers.PreTrainedTokenizerFast.from_pretrained(train_config.pretrained_model_name)\n",
    "\n",
    "## Load weights.\n",
    "model = transformers.BartForConditionalGeneration.from_pretrained(train_config.pretrained_model_name)\n",
    "model.load_state_dict(bart_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing input texts: 100%|██████████| 3052/3052 [00:01<00:00, 1938.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from dataloaders.bart_dataloader import get_datasets\n",
    "from dataloaders.bart_dataloader import TextAbstractSummarizationCollator\n",
    "\n",
    "ts_ds = get_datasets(tokenizer, fpath=Path(config.test_data_path), mode=\"test\")\n",
    "\n",
    "ts_loader = torch.utils.data.DataLoader(\n",
    "    ts_ds,\n",
    "    batch_size=config.batch_size_per_device,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=TextAbstractSummarizationCollator(\n",
    "        tokenizer=tokenizer,\n",
    "        config=config,\n",
    "        mode=\"test\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382/382 [08:04<00:00,  1.27s/it]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    if config.gpu_id >= 0:\n",
    "        model.cuda(config.gpu_id)\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    ## Don't forget turn-on evaluation mode.\n",
    "    model.eval()\n",
    "\n",
    "    outputs = []\n",
    "    for mini_batch in tqdm(ts_loader, total=len(ts_loader)):\n",
    "        id = mini_batch[\"id\"]\n",
    "        input_ids = mini_batch[\"input_ids\"]\n",
    "        attention_mask = mini_batch[\"attention_mask\"]\n",
    "\n",
    "        if config.var_len:\n",
    "            ## Variable min, max length of target summaries.\n",
    "            ## We know that summaries ~= text * 0.1.\n",
    "            avg_len = int(input_ids.ne(tokenizer.pad_token_id).view(-1).sum() / input_ids.size(0))\n",
    "            min_length = max(64,  int(avg_len * 0.05))\n",
    "            max_length = min(256, int(avg_len * 0.15))\n",
    "            ## And we don't need to set length penalty anymore.\n",
    "            config.length_penalty = 1.0\n",
    "        else:\n",
    "            min_length = config.tar_max_len // 4 ## maybe, less then 64 (e.g. 48 or 32) can be more score..\n",
    "            max_length = config.tar_max_len\n",
    "\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "        ## Generate ids of summaries.\n",
    "        output = model.generate(\n",
    "            input_ids, \n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length,                  ## maximum summarization size\n",
    "            min_length=min_length,                  ## minimum summarization size\n",
    "            early_stopping=True,                    ## stop the beam search when at least 'num_beams' sentences are finished per batch\n",
    "            num_beams=config.beam_size,             ## beam search size\n",
    "            bos_token_id=tokenizer.bos_token_id,    ## <s> = 0\n",
    "            eos_token_id=tokenizer.eos_token_id,    ## <\\s> = 1\n",
    "            pad_token_id=tokenizer.pad_token_id,    ## 3\n",
    "            length_penalty=config.length_penalty,   ## value > 1.0 in order to encourage the model to produce longer sequences\n",
    "            no_repeat_ngram_size=config.no_repeat_ngram_size,   ## same as 'trigram blocking'\n",
    "        )\n",
    "        ## If you want to decode by each sentence, you may \n",
    "        ## call 'decode' fn, not 'batch_decode'.\n",
    "        output = tokenizer.batch_decode(\n",
    "            output.tolist(), \n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "\n",
    "        ## Get all.\n",
    "        outputs.extend([{\"id\": id_, \"output\": output_} for id_, output_ in zip(id, output)])\n",
    "\n",
    "## Sort and extract.\n",
    "outputs = sorted(\n",
    "    outputs,\n",
    "    key=itemgetter(\"id\"),\n",
    "    reverse=False,\n",
    ")\n",
    "outputs = [i[\"output\"] for i in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "미래의 나의 모습  미래의 모습  현재의 모습  미래 도시에 대한 본인의 생각  미래의 나의 생각  현재의 도시에 대해 어떤 생각을 가지고 있는지  미래의 도시에 대해서 어떠한 생각을 가지고 있는지에 대한 나의 생각과 생각    나의 생각을 정리하여 미래에 대한 당신의 생각을 정리할 수 있는 기회가 될 수 있을 것이다  \n"
     ]
    }
   ],
   "source": [
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Mar 15 2022, 12:22:08) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
