{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "\n",
    "import datetime\n",
    "import easydict\n",
    "import itertools\n",
    "import json\n",
    "import matplotlib\n",
    "import pathlib\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from operator import itemgetter\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = easydict.EasyDict({\n",
    "\n",
    "    'gpu_id': 3,\n",
    "    'train_data_path': '../datasets/integrated_pre_datasets/train_data.tsv',\n",
    "    'valid_data_path': '../datasets/integrated_pre_datasets/valid_data.tsv',\n",
    "    'test_data_path': '../datasets/integrated_pre_datasets/test_data.tsv',\n",
    "    'pretrained_model_name': 'gogamza/kobart-base-v1',\n",
    "    'ckpt': \"ckpt\",\n",
    "    'logs': \"logs\",\n",
    "    'batch_size_per_device': 8, # 8이 최선\n",
    "    'gradient_accumulation_steps': 16,\n",
    "    'lr': 5e-5,\n",
    "    'weight_decay': 1e-2,\n",
    "    'warmup_ratio': .2,\n",
    "    'n_epochs': 10,\n",
    "    'inp_max_len': 1024,\n",
    "    'tar_max_len': 256,\n",
    "    'model_fpath': \"model_records/kogpt2-model.pth\",\n",
    "    'beam_size': 5,\n",
    "    \"prompt4summary\": \"1줄요약\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "saved_data = torch.load(\n",
    "    \"/workspace/home/uglee/Projects/title_extraction/src/model_records/kogpt2-model.pth\",\n",
    "    map_location=\"cpu\" if config.gpu_id < 0 else \"cuda:%d\" % config.gpu_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kogpt2_best = saved_data[\"model\"]\n",
    "train_config = saved_data[\"config\"]\n",
    "tokenizer = transformers.PreTrainedTokenizerFast.from_pretrained(train_config.pretrained_model_name)\n",
    "\n",
    "## Load weights.\n",
    "model = transformers.GPT2LMHeadModel.from_pretrained(train_config.pretrained_model_name)\n",
    "model.resize_token_embeddings( len(tokenizer) )\n",
    "\n",
    "model.load_state_dict(kogpt2_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing input texts: 100%|██████████| 3052/3052 [00:01<00:00, 1909.30it/s]\n",
      "Tokenizing input texts: 100%|██████████| 3052/3052 [00:01<00:00, 1889.98it/s]\n"
     ]
    }
   ],
   "source": [
    "from dataloaders.gpt2_dataloader import get_datasets\n",
    "from dataloaders.gpt2_dataloader import TextAbstractSummarizationCollator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "ts_ds = get_datasets(\n",
    "    tokenizer,\n",
    "    config=config,\n",
    "    fpath=Path(config.test_data_path), \n",
    "    mode=\"test\"\n",
    "    )\n",
    "\n",
    "ts_loader = DataLoader(\n",
    "    ts_ds,\n",
    "    batch_size=config.batch_size_per_device,\n",
    "    shuffle=False,\n",
    "    collate_fn=TextAbstractSummarizationCollator(\n",
    "        tokenizer=tokenizer,\n",
    "        config=config\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts <torch.utils.data.dataloader.DataLoader object at 0x7f493f97ceb0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/382 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/workspace/home/uglee/Projects/title_extraction/src/gpt2_inference.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7079746f7263685f646576222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7562756e7475227d7d/workspace/home/uglee/Projects/title_extraction/src/gpt2_inference.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mts\u001b[39m\u001b[39m\"\u001b[39m, ts_loader)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7079746f7263685f646576222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7562756e7475227d7d/workspace/home/uglee/Projects/title_extraction/src/gpt2_inference.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7079746f7263685f646576222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7562756e7475227d7d/workspace/home/uglee/Projects/title_extraction/src/gpt2_inference.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mfor\u001b[39;00m idx, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(ts_loader)):\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7079746f7263685f646576222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7562756e7475227d7d/workspace/home/uglee/Projects/title_extraction/src/gpt2_inference.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m         model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7079746f7263685f646576222c2273657474696e6773223a7b22686f7374223a227373683a2f2f7562756e7475227d7d/workspace/home/uglee/Projects/title_extraction/src/gpt2_inference.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m         input_ids \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m/workspace/home/uglee/Projects/title_extraction/src/dataloaders/gpt2_dataloader.py:188\u001b[0m, in \u001b[0;36mTextAbstractSummarizationCollator.__call__\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, samples: List[Dict[\u001b[39mstr\u001b[39m, List[\u001b[39mint\u001b[39m]]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, List[\u001b[39mint\u001b[39m]]:\n\u001b[0;32m--> 188\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_collator(samples) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_test_collator(samples)\n",
      "File \u001b[0;32m/workspace/home/uglee/Projects/title_extraction/src/dataloaders/gpt2_dataloader.py:163\u001b[0m, in \u001b[0;36mTextAbstractSummarizationCollator._train_collator\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m    158\u001b[0m labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pad(labels, token_id\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mignore_index)\u001b[39m#.astype(np.int64) ## why != \"padding_id\" ???\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39m## Pack as pre-defined arguments. See:\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39m##   https://huggingface.co/docs/transformers/model_doc/bart#transformers.BartForConditionalGeneration\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mreturn\u001b[39;00m {\n\u001b[0;32m--> 163\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m:                torch\u001b[39m.\u001b[39;49mfrom_numpy(input_ids),\n\u001b[1;32m    164\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m:           torch\u001b[39m.\u001b[39mfrom_numpy(attention_mask),\n\u001b[1;32m    165\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m:                   torch\u001b[39m.\u001b[39mfrom_numpy(labels),\n\u001b[1;32m    166\u001b[0m }\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cpu') if config.gpu_id < 0 else torch.device('cuda:%d' % config.gpu_id)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, data in enumerate(tqdm(ts_loader)):\n",
    "        model.eval()\n",
    "\n",
    "        input_ids = data['input_ids']\n",
    "        attention_mask = data['attention_mask']\n",
    "        labels = data['labels']\n",
    "\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=attention_mask\n",
    "        )#.to(self.device)\n",
    "\n",
    "        outputs = tokenizer.batch_decode(\n",
    "            outputs.tolist(),\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        outputs.extend([{\"id\": id_, \"output\": output_} for id_, output_ in zip(id, outputs)])\n",
    "\n",
    "outputs = sorted(\n",
    "    outputs,\n",
    "    key=itemgetter(\"id\"),\n",
    "    reverse=False,\n",
    ")\n",
    "outputs = [i[\"output\"] for i in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "냉이캐기 냉이 냉이이이기캐기?이네 냉이네이이캐김 냉이 두이이 캐기냉이이냉이캐캐기서 냉이김이이 제이이김기이이제 냉이어이이 냉이가  냉이\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
